{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How do you extract categories of dog food from the page (https://www.chewy.com/b/dog-288). Explain and write a python script.\n",
    "\n",
    "* write a python script to get each product url in for dog food , wet-food category (https://www.chewy.com/b/dog-288)\n",
    "\n",
    "* write a function to detect how many pages of products in a category. So given the category url the function should output the number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fake-useragent in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.1.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import requests      #send request to HTML page\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup     #python library for extracting data\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "import pandas as pd                #Further Analysis of the extracted Data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the lists to store the extracted data\n",
    "# The data that we extract is unstructured data. So we’ll create empty lists to store them in a structured form,\n",
    "count=0                  # Intialize search row count\n",
    "products=[]              #List to store name of the product\n",
    "prices=[]                #List to store price of the product\n",
    "ratings=[]               #List to store rating of the product\n",
    "#specifications = []     #List to store specifications of the product\n",
    "buscuits_cookies_crunchyTreats = []                 #List to store buscuits_cookies_crunchyTreatsspecifications of the product\n",
    "soft_chewyTreats = []                 #List to store soft_chewyTreats  specifications of the product\n",
    "dentalTreats = []                  #List to store dentalTreats specifications of the product\n",
    "jerkytteats = []                  #List to store jerkytteats specifications of the product\n",
    "bones_bullySticks_naturalChews = []             #List to store bones_bullySticks_naturalChewsspecifications of the product\n",
    "df=pd.DataFrame()        #Initialize Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fake_useragent.fake.FakeUserAgent object at 0x000002AC442BDEC8>\n"
     ]
    }
   ],
   "source": [
    "# Creating an User agent  pip insatll fake-useragent\n",
    "# A User agent acts as a bridge between the user and the internet . \n",
    "# It gives the webserver necessary information about our browser, software, device type and etc.\n",
    "# According to this information the web servers can display different webpages for us\n",
    "# The web server uses this information to adapt the content to specific web browsers and different foods specifications \n",
    "# https://pypi.org/project/fake-useragent/    \n",
    "# read here\n",
    " \n",
    "user_agent = UserAgent() # Dummy User Agent\n",
    "print(user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the product name. we are searching for laptops\n",
    "# The extracted data will be related to that product.\\ # Search for Laptops\n",
    "product_name = 'DogFood'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.chewy.com/b/dog-288\n"
     ]
    }
   ],
   "source": [
    "# Find Elements by ID\n",
    "#To extract data from multiple pages of the product listing we’re going to use a for loop.\n",
    "# The range will specify the number of pages to be extracted\n",
    "\n",
    "#url = \"https://www.chewy.com/search?q={0}&page={1}\" \n",
    "#print( url.format(products,1))           \n",
    "url=\"https://www.chewy.com/b/dog-288\"\n",
    "print(url.format(product_name,1))    #run and check this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.chewy.com/b/dog-288\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3): # Limiting search to 3 pages due to multiple redirection issues for higher number of pages\n",
    "    url = \"https://www.chewy.com/b/dog-288\" # Scrape data from chewy.com\n",
    "    url = url.format(product_name,i)\n",
    "    print(url)\n",
    "    \n",
    "    ## Getting the reponse from the page using get method of requests module\n",
    "    #page= requests.get(url, headers ={\"User_agent\": user_agent.chrome})\n",
    "    page= requests.get(url)\n",
    "    print(page)\n",
    "    \n",
    "    ## Storing the content of the page in a variable\n",
    "    html=page.content\n",
    "    print(html)\n",
    "    \n",
    "    # To Extract data from html file --- Creating BeautifulSoup object\n",
    "    page_soup=bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    print(page_soup.prettify())     #will show as a nested html file\n",
    "    #it gives the visual representation of the parse tree created from the raw HTML content.\n",
    "\n",
    "    \n",
    "    #Iterate over page_soup.find_all('p')   \n",
    "    # this will iterate over all paras\n",
    "    print(page_soup.find_all('p')[0].get_text())\n",
    "\n",
    "    \n",
    "    ## Decoding the tags\n",
    "    #('a',{'class':'_cw-desktop'})\n",
    "\n",
    "    for containers in page_soup.findAll('a',{'class':'_sfw-header__skip-link'}): \n",
    "       # <a href=\"#search-autocomplete\" class=\"sfw-header__skip-link\">Skip to search</a>\n",
    "        #<a href=\"#search-autocomplete\" class=>Skip to search</a>\n",
    "        name=containers.find('div', attrs={'class':'department'})\n",
    "        \n",
    "        price=containers.find('div', attrs={'class':'bx-wrapper'})\n",
    "        \n",
    "        rating=containers.find('nav', attrs={'class':'sfw-header_wrapper chirp-sfw'})\n",
    "        \n",
    "        specification = containers.find('div', attrs={'class':'cms-include'})\n",
    "        \n",
    "        ## Splitting integrated specification into individual CPU, RAM, OS, HDD and Display specifications\n",
    "        for col in specification:\n",
    "            col=col.find_all('a', attrs={'class':'cw-desktop'})\n",
    "            buscuits_cookies_crunchyTreats = col[0].text                #List to store buscuits_cookies_crunchyTreatsspecifications of the product\n",
    "            soft_chewyTreats =col[1].text              #List to store soft_chewyTreats  specifications of the product\n",
    "            dentalTreats = col[2].text                  #List to store dentalTreats specifications of the product\n",
    "            jerkytteats = col[3].text                  #List to store jerkytteats specifications of the product\n",
    "            bones_bullySticks_naturalChews = col[4].text            #List to store bones_bullySticks_naturalChewsspecifications of the product\n",
    "            \n",
    "            \n",
    "        \n",
    "        products.append(Name.text) # Add product name to list\n",
    "        \n",
    "        prices.append(price.text) # Add price to list\n",
    "        \n",
    "        #specifications.append(specification.text) if type(specification) == bs4.element.Tag  else specifications.append('NaN')\n",
    "        \n",
    "        bsc.append(bis) # Add buscuits_cookies_crunchyTreats specifications to list\n",
    "        \n",
    "        sct.append(che) # Add soft_chewyTreats specifications to list\n",
    "        \n",
    "        det.append(den) # Add dentalTreats specifications to list\n",
    "        \n",
    "        jet.append(jer) # Add jerkytteatsspecifications to list\n",
    "        \n",
    "        bls.append(bul) # Add bones_bullySticks_naturalChewsspecifications to list\n",
    "        \n",
    "        rat.append(rating.text) if type(rating) == bs4.element.Tag  else ratings.append('NaN') # Add Rating to list\n",
    "        \n",
    "        count=count+1 # Increment row count\n",
    "    \n",
    "    ## Create a dataframe with structured data from all searched rows\n",
    "    df = pd.DataFrame({'product_name':products,'buscuits_cookies_crunchyTreats':bsc,'soft_chewyTreats':sct,'dentalTreats':det,'jerkytteats':jet,'bones_bullySticks_naturalChews':bls,'Price':prices,'ratings':rat,})\n",
    "\n",
    "print('No. of rows searched',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For extracting data from soup we need to specify the html tags we want to retrieve the data from.\n",
    "#We could use inspect element on the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Recap of the html tags\n",
    "*p - A paragraph of text\n",
    "\n",
    "*h1- A top-level heading\n",
    "\n",
    "*h2, h3 - A lower-level heading\n",
    "\n",
    "*li- An item in a list\n",
    "\n",
    "*img - An image\n",
    "\n",
    "*tr- A row in a table\n",
    "\n",
    "*td - A cell in a table\n",
    "\n",
    "*a - A link\n",
    "\n",
    "*div - A block of space on the page (generic)\n",
    "\n",
    "*span - A portion of text on the page (generic)\n",
    "\n",
    "*meta - Information about the page that is not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### find() and find_all() function in Beautiful Soup\n",
    "* To extract a single tag, we can instead use the find_all method, which will find all the instances of a tag on a page\n",
    "* soup.find_all('p')    # this iwll iterate over all paras* soup.find_all('p')[0].get_text()\n",
    " \n",
    "* Classes and ids are used by CSS to determine which HTML elements to apply certain styles to.\n",
    "* We can also use them when scraping to specify specific elements we want to scrape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head() # Preview dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail() # Preview dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # Check for 'NaN' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # Dataframe Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # Describe Data before cleaning and dtype conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # Check data types of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Price column to remove ₹ and delimiter ',' used for the thousandth place \n",
    "df['Price'] = df['Price'].str.lstrip('₹')\n",
    "df['Price'] = df['Price'].replace({',' : ''}, regex=True)\n",
    "df.head() # Check if formatting is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Product Name'] = 'DogFood'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df.describe() # Describe Data after cleaning and dtype conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(a, axis=1) # Drop rows with wrongly positioned data elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Price column to remove ₹ and delimiter ',' used for the thousandth place \n",
    "df['Price'] = df['Price'].str.lstrip('₹')\n",
    "df['Price'] = df['Price'].replace({',' : ''}, regex=True)\n",
    "df.head() # Check if formatting is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric columns in string format to float for mathematical and graphic operations\n",
    "#for i in range(6,8,1):\n",
    "#df.iloc[:,i]= df.iloc[:,i].astype(float).copy()\n",
    "\n",
    "df.iloc[:,6]= df.iloc[:,6].astype(float)\n",
    "df.iloc[:,7]= df.iloc[:,7].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # Check data types of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
